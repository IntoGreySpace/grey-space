+++
Categories = ["Brains"]
bibfile = "ccnlab.json"
+++

**Epistemology** is the branch of philosophy concerned with the nature of human knowledge. What is _truth_, _understanding_, _meaning_, etc?

There are many different approaches that have been taken to this most central of philosophical questions, going back to the ancient Greeks etc. See [Wikipedia](https://en.wikipedia.org/wiki/Epistemology) for an overview.

The approach taken here is based on the following principles:

1. The only _primary_ truth is [[subjective]]. This is the only think we can know for sure, without any further contingencies.

2. Across many people's brains, across time, there is strong evidence for an [[objective]] reality outside of our subjective experience. However, our own individual knowledge of this objective external world is only obtained through our own subjective experience, so it is necessarily secondary to that primary subjective experience. In other words, the objective world and everything we believe to be true about it is **fundamentally contingent** on the subjective state of each individual's brain -- there is no "direct", "absolute", "pure" access to any "true" facts about the objective external world.

3. Distinct individual humans (or other similarly "intelligent" agents) _by definition_ can only _share_ knowledge about this external objective world, because the nature of subjective experience is, _by definition_, subjective and thus only available exclusively to each individual.

4. Therefore, all sharable, objective knowledge is fundamentally contingent on a **stack of assumptions** that must also be shared by the individuals entering into such a sharing relationship. The truth value of these assumptions is not directly verifiable, because they are assumptions about the nature of objective reality. 

5. The "validity" (truth value) of these assumptions derives then from the **work** that they accomplish in helping individuals come to a **mutually consistent** understanding of the objective world. For example, the [[scientific method]] has proven its value as a procedure for generating a consistent body of understanding across individuals, across time.

6. By way of analogy, the validity of this stack of assumptions is based on a kind of _end to end backpropagation_ process where many people keep using the same stack across time and across different domains of understanding, and if these assumptions continue to enable the development of mutually consistent, stable, "individually satisfying" levels of understanding across these individuals, across time, then the effective "truth value" of these assumptions increases.

    But, per above, there is no point at which they obtain anything like "absolute truth". Instead, it is essential that each individual recognize the presence and role of this stack of assumptions in shaping their subjective feelings of understanding and knowledge, and fundamentally acknowledge that these assumptions are not in any way independently valid beyond their evident utility as defined above.

So what is this stack of assumptions? Well, the first entries are articulated above. Yep, this is a fully recursive enterprise. You gotta start somewhere.

Some other important tools in the assumption stack are:

* **Parsimony**: simpler explanations that account for the same amount of data are generally to be favored, because they are likely to _generalize_ better to novel situations. A classic example is the heliocentric model of the solar system, which is fundamentally very simple, compared to the _epicycle_ theory, which required lots of complex assumptions and corrections, to account for the same data.

* **Levels of analysis**: It is possible to describe the same [[physical reality]] at multiple different levels of abstraction or analysis. Underlyingly, it is most parsimonious to assume that there is just a kind of bare physical reality, operating independently and autonomously according to the most fundamental laws of physics. Everything else is just an abstraction that **we impose onto this bare reality** in order to understand it more efficiently and effectively. The principle of [[emergence]] and associated discussion there is essential for understanding the "reality" of emergent levels above the most fundamental physics level.

## Truthy

TODO: Figure from Alex's slides about abstraction, etc.


## Empirical epistemology

If you follow the full stack of assumptions to its "logical" conclusion, the only way to really understand the nature of human understanding is to understand in detail how the human brain functions, because that is where our knowledge somehow emerges. This then defines a purely empirical [[objective]] domain of epistemology, as opposed to the philosophical stack of assumptions outlined above.

According to the current state of cognitive neuroscience, e.g., as detailed in [compcogneuro](https://compcogneuro.org), the brain is made of neurons, which function by integrating and sending chemical and electrical signals to each other. Current "AI" models are based on the same principles of neural computation, and demonstrate the _functionalist_ principle that systems of such neuron-like processing elements can capture essential features of cognition, whether they are implemented in silicon and software, or lipids, proteins and ions. The [[emergence|emergent]] behavior can be the same, despite differences in the underlying hardware. This is another vote in favor of the power of levels of analysis and abstractions.

From an epistemological standpoint, the critical thing about networks of neurons is that they are _not_ based on any kind of formal logic! Thus, human understanding is fundamentally not based on rationality.


